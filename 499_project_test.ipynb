{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-youn/499_Project/blob/main/499_project_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RAw_eqYMTHoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab34fe0-a7e6-4182-ba1c-b4633bf995b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of vocab: 42917\n",
            "size of text: 130741\n",
            "size of grapheme_pairs: 28870\n",
            "[['Wise men alway', 'Affirm and say'], [\"That best 'tis for a man\", 'The business that he can'], ['Diligently', 'For to apply'], ['And in no wise', 'To enterprise'], ['Another faculty', 'Is never like to thrive'], ['For he that will', 'And can no skill'], ['He that hath left', \"The hosier's craft\"], ['And falleth to making shone', 'His thrift is well-nigh done'], ['The smith that shall', 'To painting fall'], ['A black draper', 'With white paper'], ['To go to writing school', 'I ween shall prove a fool'], ['An old butler', 'Become a cutler'], ['And an old trot', 'That can God wot'], ['Nothing but kiss the cup', 'Till she have soused him up'], ['With her physic', 'Will keep one sick'], ['A man of law', 'That never saw'], ['The ways to buy and sell', 'I pray God speed him well'], ['Weening to rise', 'By merchandise'], ['A merchant eke', 'That will go seek'], ['By all the means he may', 'His money clean away'], ['To fall in suit', 'Till he dispute'], ['Pleading the law', 'For every straw'], ['Shall prove a thrifty man', 'I cannot tell you whan'], [\"With 'bate and strife\", 'But by my life'], ['When an hatter', 'Will go smatter']]\n"
          ]
        }
      ],
      "source": [
        "# Download corpus\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/sravanareddy/rhymedata/master/english_raw/-.txt\"\n",
        "authors = [\n",
        "    \"more\",\"wyatt\",\"constable\",\"daniel\",\"drayton\",\"fletcher\",\"griffin\",\"jonson\",\"lodge\",\n",
        "    \"lovelace\",\"milton\",\"shakespeare\",\"sidney\",\"spenser\",\"smith\",\"dryden\",\"finch\",\"pope\",\n",
        "    \"prior\",\"swift\",\"byron\",\"coleridge\",\"goldsmith\",\"shelley\",\"turner\",\"wordsworth\",\n",
        "    \"brooke\",\"chesterton\",\"crosland\",\"housman\",\"kipling\",\"thomas\",\n",
        "    ]\n",
        "text = \"\"\n",
        "for author in authors:\n",
        "  newurl = url.replace('-',author)\n",
        "  response = requests.get(newurl)\n",
        "  response.raise_for_status() # Raise an exception for invalid HTTP status codes\n",
        "  text += \"\\n\" + response.text\n",
        "  # line_pairs.extend(parse_file(text_data))\n",
        "\n",
        "# parse corpus and remove puncuation\n",
        "text = text.replace(',','').replace('.','').replace('?','').replace('!','').replace(';','').replace(':','').replace('\"','')\n",
        "text = text.replace('  ', ' ')\n",
        "text = text.split(\"\\n\")\n",
        "for i in range(len(text)):\n",
        "  text[i] = text[i].strip()\n",
        "\n",
        "# initialize vocab set\n",
        "# and make line pairs of rhyming lines\n",
        "vocab = set()\n",
        "grapheme_pairs = []\n",
        "for i in range(len(text)):\n",
        "  line = text[i]\n",
        "  if (line[:5] != \"RHYME\") or (text[i+1] != \"\"): # if not start of new poem\n",
        "    continue\n",
        "  line = line.split(\" \")\n",
        "  map = defaultdict(list)\n",
        "  for j in range(1,len(line)): # get indices for each kind of rhyming lines\n",
        "    map[line[j]].append(j)\n",
        "    for w in text[i+j+1].split(' '): # add all words to vocab\n",
        "      vocab.add(w)\n",
        "\n",
        "  for vec in map.values(): # for each rhyming lines\n",
        "    # print(vec)\n",
        "    for k in range(len(vec)-1): # each pair of lines that rhyme\n",
        "      # TODO: optional: can remove for more data\n",
        "      if k%2 == 1: # no duplicates\n",
        "        continue\n",
        "      # grapheme_pairs.append([text[i+vec[k]+1].split(' '),text[i+vec[k]+2].split(' ')])\n",
        "      grapheme_pairs.append([text[i+vec[k]+1],text[i+vec[k+1]+1]])\n",
        "      # grapheme_pairs.append([text[i+vec[k]+1].split(' ')[-1],text[i+vec[k]+2].split(' ')[-1]])\n",
        "\n",
        "print(\"size of vocab:\", len(vocab))\n",
        "print(\"size of text:\", len(text))\n",
        "print(\"size of grapheme_pairs:\", len(grapheme_pairs))\n",
        "print(grapheme_pairs[:25])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "meA6gXByujbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f35443-1d01-438c-f801-ebc03984a8c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.23.0 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.23.0->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.23.0 python-Levenshtein-0.23.0 rapidfuzz-3.5.2\n",
            "Collecting g2p_en\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from g2p_en) (1.23.5)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p_en) (3.8.1)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from g2p_en) (7.0.0)\n",
            "Collecting distance>=0.1.3 (from g2p_en)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect>=0.3.1->g2p_en) (1.10.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from inflect>=0.3.1->g2p_en) (4.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p_en) (4.66.1)\n",
            "Building wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16256 sha256=c3306f8d0cd6b88b62f02e6f4b1f6787ddf2cf8e46b6605f4de81ec25661e924\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "Successfully built distance\n",
            "Installing collected packages: distance, g2p_en\n",
            "Successfully installed distance-0.1.3 g2p_en-2.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein\n",
        "!pip install g2p_en\n",
        "import itertools\n",
        "import Levenshtein\n",
        "import heapq\n",
        "from g2p_en import G2p\n",
        "import nltk\n",
        "from nltk.corpus import cmudict\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('cmudict')\n",
        "phoneme_dict = cmudict.dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jdHgUcIjuqsP"
      },
      "outputs": [],
      "source": [
        "# G2P WORD TO PHONEME FUNCTIONS\n",
        "def gp_word_to_phonemes(text):\n",
        "    g2p = G2p()\n",
        "    phonemes = g2p(text)\n",
        "    return ' '.join(phonemes)\n",
        "\n",
        "def gp_list_to_phonemes(words_list):\n",
        "    phonemes_list = []\n",
        "    for word in words_list:\n",
        "        phonemes = gp_word_to_phonemes(word)\n",
        "        phonemes_list.append(phonemes)\n",
        "\n",
        "    return phonemes_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aVXtc1Mwuscp"
      },
      "outputs": [],
      "source": [
        "# CMU DICT WORD TO PHONEME FUNCTIONS\n",
        "def cmu_word_to_phonemes(word):\n",
        "    word = word.lower()\n",
        "    phonemes = phoneme_dict.get(word)\n",
        "\n",
        "    if phonemes:\n",
        "        # Choosing the first pronunciation variant\n",
        "        return ' '.join(phonemes[0])\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def cmu_list_to_phonemes(words_list):\n",
        "    phonemes_list = []\n",
        "\n",
        "    for word in words_list:\n",
        "        phonemes = cmu_word_to_phonemes(word)\n",
        "        phonemes_list.append(phonemes)\n",
        "\n",
        "    return phonemes_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UhAThGjTHlb",
        "outputId": "ca27fd15-e9bd-4130-aeaf-0c697f44c127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of words in dictionary: 14800\n"
          ]
        }
      ],
      "source": [
        "# see how many words are in actual dictionary\n",
        "\n",
        "count = 0\n",
        "phoneme_dict_keys = phoneme_dict.keys()\n",
        "for word in vocab:\n",
        "  if word in phoneme_dict_keys:\n",
        "    count += 1\n",
        "\n",
        "print(\"number of words in dictionary:\", count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IGTirYhTHgF",
        "outputId": "74951cbe-e16d-4d5e-9abb-5e090ff18ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sweetly\n",
            "S W IY1 T L IY0\n",
            "faire-built\n",
            "F AY1 R B W IH2 L T\n"
          ]
        }
      ],
      "source": [
        "# create grapheme to phoneme dict for our vocab\n",
        "for word in vocab:\n",
        "  if word in phoneme_dict_keys:\n",
        "    print(word)\n",
        "    print(cmu_word_to_phonemes(word))\n",
        "    break\n",
        "\n",
        "for word in vocab:\n",
        "  if word != \"\" and word not in phoneme_dict_keys:\n",
        "    print(word)\n",
        "    print(gp_word_to_phonemes(word))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "haO_yJ7QITKb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjwrMQLAzuKx",
        "outputId": "0eecbf59-28d8-4966-f11b-68374e9c2c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 433/42917 [04:46<9:19:25,  1.27it/s]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "g_to_p_dict = {}\n",
        "\n",
        "# map each word in vocab to its phoneme representation\n",
        "for word in tqdm(vocab):\n",
        "    if word in phoneme_dict_keys:\n",
        "        g_to_p_dict[word] = cmu_word_to_phonemes(word)\n",
        "    else:\n",
        "        g_to_p_dict[word] = gp_word_to_phonemes(word)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tvzIPJIpQeX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKUv-MLuTHZ0"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "phoneme_pairs = copy.deepcopy(grapheme_pairs)\n",
        "for pair in phoneme_pairs:\n",
        "  for line in pair:\n",
        "    for word in line\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csfnT1B7THSR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaFOJUPrDsHmEO7+pA5o8q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}